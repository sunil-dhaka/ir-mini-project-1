{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please uncomment and run these if your machine does not have needed nltk data'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,re,time\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "\n",
    "\"\"\"please uncomment and run these if your machine does not have needed nltk data\"\"\"\n",
    "# nltk.download('punkt') \n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic info retrival about corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8635"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total document files in our corpus\n",
    "len(os.listdir('english-corpora'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all of them are .txt files\n",
    "all('.txt' in t for t in os.listdir('english-corpora'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D00521.txt', 'P_1709.txt', 'D00920.txt', 'T00143.txt', 'P03064.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('english-corpora')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_corpora_files=os.listdir('english-corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks to do\n",
    "\n",
    "- [x] TODO: how to remove HTML info from these pages\n",
    "- [x] TODO: how to use porter's stemmer\n",
    "- [x] TODO: how to do tokenization\n",
    "- [x] TODO: how to clean text files: remove non-ascii and what else?; and when removing non-ascii should I remove just one character or whole word\n",
    "- [x] TODO: don't remove stopwords; at least for tf-idf it does not matter; for binary and BM25??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpora_dir='test-corpora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_name):\n",
    "    '''\n",
    "    create folder with name given\n",
    "    '''\n",
    "    try:\n",
    "        os.mkdir(folder_name)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def corpora_processor(corpora_files_dir,processed_corpora_dir):\n",
    "    '''\n",
    "    Input: corpora files dir; processed folder name\n",
    "\n",
    "    Output: processed corpora files\n",
    "    '''\n",
    "    # TODO: add stemmer specifing functionality\n",
    "    create_folder(processed_corpora_dir)\n",
    "\n",
    "    porter=PorterStemmer()\n",
    "\n",
    "    for file in os.listdir(corpora_files_dir):\n",
    "        with open(corpora_files_dir+'/'+file,'r') as f:\n",
    "            corpora_file_str=f.read() # text str\n",
    "\n",
    "            \"\"\"remove css code lines\"\"\"\n",
    "            css_regex=re.compile(r'.mw.*}')\n",
    "            # substitue regex expression by ''\n",
    "            corpora_file_str=css_regex.sub('',corpora_file_str)\n",
    "\n",
    "            \"\"\"remove html tag lines\"\"\"\n",
    "            html_regex=re.compile(r'<.*>')\n",
    "            # substitue regex expression by ''\n",
    "            corpora_file_str=html_regex.sub('',corpora_file_str)\n",
    "\n",
    "            \"\"\"tokenize file str\"\"\"\n",
    "            # here each doc could be processed line by line; i am doing whole doc str\n",
    "            tokens=word_tokenize(corpora_file_str)\n",
    "            \n",
    "            \"\"\"stem file tokens\"\"\"\n",
    "            processed_corpora_file_str=''\n",
    "            for token in tokens:\n",
    "                # avoid non-ascii as suggested in the question\n",
    "                if token.isascii():\n",
    "                    processed_corpora_file_str+=porter.stem(token)\n",
    "                    processed_corpora_file_str+=' ' # to seperate processed tokens\n",
    "\n",
    "        \"\"\"store processed file with same name\"\"\"\n",
    "        with open(processed_corpora_dir+'/'+file,'w') as output_file:\n",
    "            output_file.write(processed_corpora_file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- time taken for whole corpora is too much. around 1600 sec or 25 min\n",
    "    - one solution is to directly stem file string without tokenization; so we stem whole string together rather than each token and after stemming we tokenize and store; it could save half time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing processor on test-corpora of 100 files\n",
    "corpora_processor(test_corpora_dir,'processed-test-corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10640\n"
     ]
    }
   ],
   "source": [
    "# tokenization of a file withput using word_tokenizer from nltk\n",
    "with open('test-corpora/C00001.txt','r') as doc:\n",
    "    file=doc.read()\n",
    "    regex_exp3=re.compile(r'\\n') # to sub \\n by ' '\n",
    "    regex_exp4=re.compile(r'\\t') # to sub \\n by ''\n",
    "    clean_text=regex_exp3.sub(' ',file)\n",
    "    clean_text=regex_exp4.sub('',clean_text)\n",
    "    tokens=clean_text.split(' ')\n",
    "    print(len(tokens))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "365d70965140afb04a698773bfdd31483bc82432b779112c2a78b5de7c16d125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
