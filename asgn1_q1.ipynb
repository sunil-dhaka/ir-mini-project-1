{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please uncomment and run these if your machine does not have needed nltk data'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,re,time\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "\n",
    "\"\"\"please uncomment and run these if your machine does not have needed nltk data\"\"\"\n",
    "# nltk.download('punkt') \n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic info retrival about corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8635"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total document files in our corpus\n",
    "len(os.listdir('english-corpora'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all of them are .txt files\n",
    "all('.txt' in t for t in os.listdir('english-corpora'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D00521.txt', 'P_1709.txt', 'D00920.txt', 'T00143.txt', 'P03064.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('english-corpora')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_corpora_files=os.listdir('english-corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks to do\n",
    "\n",
    "- [x] TODO: how to remove HTML info from these pages\n",
    "- [x] TODO: how to use porter's stemmer\n",
    "- [x] TODO: how to do tokenization\n",
    "- [x] TODO: how to clean text files: remove non-ascii and what else?; and when removing non-ascii should I remove just one character or whole word\n",
    "- [x] TODO: don't remove stopwords; at least for tf-idf it does not matter; for binary and BM25??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpora_dir='test-corpora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_name):\n",
    "    '''\n",
    "    create folder with name given\n",
    "    '''\n",
    "    try:\n",
    "        os.mkdir(folder_name)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "def corpora_processor(corpora_files_dir,processed_corpora_dir):\n",
    "    '''\n",
    "    Input: corpora files dir; processed folder name\n",
    "\n",
    "    Output: processed corpora files\n",
    "    '''\n",
    "    # TODO: add stemmer specifing functionality\n",
    "    create_folder(processed_corpora_dir)\n",
    "\n",
    "    porter=PorterStemmer()\n",
    "\n",
    "    for file in os.listdir(corpora_files_dir):\n",
    "        with open(corpora_files_dir+'/'+file,'r') as f:\n",
    "            corpora_file_str=f.read() # text str\n",
    "\n",
    "            \"\"\"remove css code lines\"\"\"\n",
    "            css_regex=re.compile(r'.mw.*}')\n",
    "            # substitue regex expression by ''\n",
    "            corpora_file_str=css_regex.sub('',corpora_file_str)\n",
    "\n",
    "            \"\"\"remove html tag lines\"\"\"\n",
    "            html_regex=re.compile(r'<.*>')\n",
    "            # substitue regex expression by ''\n",
    "            corpora_file_str=html_regex.sub('',corpora_file_str)\n",
    "\n",
    "            \"\"\"tokenize file str\"\"\"\n",
    "            # here each doc could be processed line by line; i am doing whole doc str\n",
    "            tokens=word_tokenize(corpora_file_str)\n",
    "            \n",
    "            \"\"\"stem file tokens\"\"\"\n",
    "            processed_corpora_file_str=''\n",
    "            for token in tokens:\n",
    "                # avoid non-ascii as suggested in the question\n",
    "                if token.isascii():\n",
    "                    processed_corpora_file_str+=porter.stem(token)\n",
    "                    processed_corpora_file_str+=' ' # to seperate processed tokens\n",
    "\n",
    "        \"\"\"store processed file with same name\"\"\"\n",
    "        with open(processed_corpora_dir+'/'+file,'w') as output_file:\n",
    "            output_file.write(processed_corpora_file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- time taken for whole corpora is too much. around 1600 sec or 25 min\n",
    "    - one solution is to directly stem file string without tokenization; so we stem whole string together rather than each token and after stemming we tokenize and store; it could save half time \n",
    "    - NOTE: above solution is not implementable using Porter since it stems token wise meaning word by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing processor on test-corpora of 100 files\n",
    "corpora_processor(test_corpora_dir,'processed-test-corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10640\n"
     ]
    }
   ],
   "source": [
    "# tokenization of a file withput using word_tokenizer from nltk\n",
    "with open('test-corpora/C00001.txt','r') as doc:\n",
    "    file=doc.read()\n",
    "    regex_exp3=re.compile(r'\\n') # to sub \\n by ' '\n",
    "    regex_exp4=re.compile(r'\\t') # to sub \\n by ''\n",
    "    clean_text=regex_exp3.sub(' ',file)\n",
    "    clean_text=regex_exp4.sub('',clean_text)\n",
    "    tokens=clean_text.split(' ')\n",
    "    print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_word_freq(word_list):\n",
    "    '''\n",
    "    takes word list and returns unique-words with their freq in dict\n",
    "    '''\n",
    "    unqiue_words=list()\n",
    "    words_freq={}\n",
    "    # store unqiue words in list\n",
    "    for word in word_list:\n",
    "        if word not in unqiue_words:\n",
    "            unqiue_words.append(word)\n",
    "    # get unique words freq from all words list using count\n",
    "    for unqiue_word in unqiue_words:\n",
    "        # NOTE: this same method can be used to get freq of a particular word in given document\n",
    "        words_freq[unqiue_word]=word_list.count(unqiue_word) # used count method of lists\n",
    "    \n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement pre-processing using nested dicts rather than linked lists\n",
    "doc1='from pyIR.utils.cache import Cache from pyIR.utils.collections import TweakedCounter from pyIR.utils.inverted_index import InvertedIndex'\n",
    "doc2='from collections import Counter from math import log from typing import TweakedCounter TweakedCounter'\n",
    "doc3='math from TweakedCounter TweakedCounter math import Cache'\n",
    "words_global=list() # to store words from all docs\n",
    "words_freq_global_dict={} # to store word:freq from all docs\n",
    "indexed_files={}\n",
    "DICT={}\n",
    "for i,doc in enumerate([doc1,doc2,doc3]):\n",
    "    indexed_files[i+1]=f'doc{i+1}'\n",
    "    tokens=doc.split(' ')\n",
    "    doc_word_freq=get_unique_word_freq(tokens)\n",
    "    # print(doc_word_freq)\n",
    "    for word in doc_word_freq.keys():\n",
    "        does_word_exists=DICT.get(word,None)\n",
    "        if does_word_exists is not None:\n",
    "            DICT[word][i+1]=doc_word_freq[word]\n",
    "        else:\n",
    "            # here first we create a new element in parent dict\n",
    "            DICT[word]={}\n",
    "            DICT[word][i+1]=doc_word_freq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': {1: 3, 2: 3, 3: 1},\n",
       " 'pyIR.utils.cache': {1: 1},\n",
       " 'import': {1: 3, 2: 3, 3: 1},\n",
       " 'Cache': {1: 1, 3: 1},\n",
       " 'pyIR.utils.collections': {1: 1},\n",
       " 'TweakedCounter': {1: 1, 2: 2, 3: 2},\n",
       " 'pyIR.utils.inverted_index': {1: 1},\n",
       " 'InvertedIndex': {1: 1},\n",
       " 'collections': {2: 1},\n",
       " 'Counter': {2: 1},\n",
       " 'math': {2: 1, 3: 2},\n",
       " 'log': {2: 1},\n",
       " 'typing': {2: 1}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'doc1', 2: 'doc2', 3: 'doc3'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': {'c': 3}}\n",
      "{'c': 3, 'b': 2}\n",
      "{'d': {'c': 3, 'b': 2}, 'f': {'g': 1}}\n"
     ]
    }
   ],
   "source": [
    "# rough work to see if it works\n",
    "b={'d':{'c':3}}\n",
    "print(b)\n",
    "b['d']['b']=2\n",
    "b['f']={}\n",
    "b['f']['g']=1\n",
    "print(b.get('d',None))\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "365d70965140afb04a698773bfdd31483bc82432b779112c2a78b5de7c16d125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
