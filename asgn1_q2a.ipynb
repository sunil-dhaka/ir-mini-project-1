{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep these words unchangable we use python set\n",
    "# as our docs are in english we use english stopwords\n",
    "Stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_word_freq(word_list):\n",
    "    '''\n",
    "    takes word list and returns unique-words with their freq in dict\n",
    "    '''\n",
    "    unqiue_words=list()\n",
    "    words_freq={}\n",
    "    # store unqiue words in list\n",
    "    for word in word_list:\n",
    "        if word not in unqiue_words:\n",
    "            unqiue_words.append(word)\n",
    "    # get unique words freq from all words list using count\n",
    "    for unqiue_word in unqiue_words:\n",
    "        # NOTE: this same method can be used to get freq of a particular word in given document\n",
    "        words_freq[unqiue_word]=word_list.count(unqiue_word) # used count method of lists\n",
    "    \n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"to init a node with docID and word_freq in that document\"\"\"\n",
    "\n",
    "    def __init__(self,doc_id,word_freq=None):\n",
    "        self.doc_id=doc_id\n",
    "        self.word_freq=word_freq\n",
    "        self.next=None\n",
    "\n",
    "class LinkedList:\n",
    "    '''\n",
    "    init linked list class\n",
    "    '''\n",
    "    def __init__(self,head=None):\n",
    "        self.head=head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpora_dir='test-corpora'\n",
    "english_corpora_dir='english-corpora'\n",
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:29: DeprecationWarning: invalid escape sequence \\d\n"
     ]
    }
   ],
   "source": [
    "words_global=list() # to store words from all docs\n",
    "words_freq_global_dict={} # to store word:freq from all docs\n",
    "indexed_files={}\n",
    "\n",
    "for i,file in enumerate(os.listdir(test_corpora_dir)):\n",
    "    with open(test_corpora_dir+'/'+file,'r') as f:\n",
    "        corpora_file_str=f.read() # text str\n",
    "\n",
    "    \"\"\"remove css code lines\"\"\"\n",
    "    css_regex=re.compile(r'.mw.*}')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=css_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove html tag lines\"\"\"\n",
    "    html_regex=re.compile(r'<.*>')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=html_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove special characters and only keep a-z;A-Z;0-9;space\"\"\"\n",
    "    '''\n",
    "    to avoid deprecation warning used one more backslash to escape backslash\n",
    "    so \\s -> \\\\s\n",
    "    '''\n",
    "    special_regex=re.compile('[^a-zA-Z0-9\\\\s]')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=special_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove digits for file\"\"\"\n",
    "    digit_regex=re.compile('\\d')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=digit_regex.sub('',corpora_file_str)\n",
    "\n",
    "    sentence_tokens=sent_tokenize(corpora_file_str)\n",
    "    # print(len(sentence_tokens))\n",
    "    word_tokens=word_tokenize(corpora_file_str)\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    \"\"\"avoid single characters and lower them and remove stopwords\"\"\"\n",
    "    # TODO: what about special cases like UP > up or PIN > pin \n",
    "    # TODO: incase of tf-idf stopwords do not matter. \n",
    "    # for this should keep them? for cases \"like to be or not to be\"\n",
    "    word_tokens=[porter.stem(word.lower()) for word in word_tokens if len(word)>1 and word not in Stopwords]\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    words_freq_global_dict.update(get_unique_word_freq(word_tokens)) # used update method of dict\n",
    "\n",
    "    indexed_files[i+1]=file\n",
    "\n",
    "unique_words_global=set(words_freq_global_dict.keys()) # used set to keep words unchangable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create inverted index linked list with freq count\"\"\"\n",
    "inverted_index_data={}\n",
    "for word in unique_words_global:\n",
    "    inverted_index_data[word]=LinkedList()\n",
    "    inverted_index_data[word].head=Node(1,Node)\n",
    "\n",
    "for i,file in enumerate(os.listdir(test_corpora_dir)):\n",
    "    with open(test_corpora_dir+'/'+file,'r') as f:\n",
    "        corpora_file_str=f.read() # text str\n",
    "\n",
    "    \"\"\"remove css code lines\"\"\"\n",
    "    css_regex=re.compile(r'.mw.*}')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=css_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove html tag lines\"\"\"\n",
    "    html_regex=re.compile(r'<.*>')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=html_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove special characters and only keep a-z;A-Z;0-9;space\"\"\"\n",
    "    '''\n",
    "    to avoid deprecation warning used one more backslash to escape backslash\n",
    "    so \\s -> \\\\s\n",
    "    '''\n",
    "    special_regex=re.compile('[^a-zA-Z0-9\\\\s]')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=special_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove digits for file\"\"\"\n",
    "    digit_regex=re.compile('\\d')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=digit_regex.sub('',corpora_file_str)\n",
    "\n",
    "    sentence_tokens=sent_tokenize(corpora_file_str)\n",
    "    # print(len(sentence_tokens))\n",
    "    word_tokens=word_tokenize(corpora_file_str)\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    \"\"\"avoid single characters and lower them and remove stopwords\"\"\"\n",
    "    # TODO: what about special cases like UP > up or PIN > pin \n",
    "    # TODO: incase of tf-idf stopwords do not matter. \n",
    "    # for this should keep them? for cases \"like to be or not to be\"\n",
    "    word_tokens=[porter.stem(word.lower()) for word in word_tokens if len(word)>1 and word not in Stopwords]\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    tmp_word_freq_of_doc=get_unique_word_freq(word_tokens)\n",
    "\n",
    "    for word in tmp_word_freq_of_doc.keys():\n",
    "        tmp_LinkedList=inverted_index_data[word].head\n",
    "        while tmp_LinkedList.next is not None:\n",
    "            tmp_LinkedList=tmp_LinkedList.next\n",
    "        tmp_LinkedList.next=Node(i+1,tmp_word_freq_of_doc[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word > sunil < is not found in any document\n",
      "ram and ram and sunil is present in 24 files and they are \n",
      " ['C00059.txt', 'C00052.txt', 'C00050.txt', 'C00027.txt', 'C00030.txt', 'C00042.txt', 'C00012.txt', 'C00078.txt', 'C00011.txt', 'C00015.txt', 'C00094.txt', 'C00095.txt', 'C00068.txt', 'C00049.txt', 'C00071.txt', 'C00016.txt', 'C00086.txt', 'C00066.txt', 'C00021.txt', 'C00006.txt', 'C00018.txt', 'C00073.txt', 'C00090.txt', 'C00053.txt']\n"
     ]
    }
   ],
   "source": [
    "query_input=input('Query > ')\n",
    "tokenized_query=[word.lower() for word in word_tokenize(query_input)]\n",
    "\n",
    "\n",
    "\"\"\"seperate logic and search terms\"\"\"\n",
    "bool_words=list()\n",
    "search_words=list()\n",
    "\n",
    "for word in tokenized_query:\n",
    "    if word != \"and\" and word != \"or\" and word != \"not\":\n",
    "        search_words.append(word)\n",
    "    else:\n",
    "        bool_words.append(word)\n",
    "\n",
    "\n",
    "total_documents=len(indexed_files)\n",
    "\n",
    "query_word_zero_one=list()\n",
    "for word in search_words:\n",
    "    if word in unique_words_global:\n",
    "        tmp_zero_one=[0]*total_documents\n",
    "        curr_linkedlist=inverted_index_data[word].head\n",
    "        while curr_linkedlist.next is not None:\n",
    "            tmp_zero_one[curr_linkedlist.next.doc_id-1]=1\n",
    "            curr_linkedlist=curr_linkedlist.next\n",
    "        query_word_zero_one.append(tmp_zero_one)\n",
    "    else:\n",
    "        print(f'word > {word} < is not found in any document')\n",
    "        # sys.exit() \n",
    "\"\"\"create a merged boolean(zero-one) list using bitwise operations\"\"\"\n",
    "# try:\n",
    "for word in bool_words:\n",
    "    zero_one_list1=query_word_zero_one[0]\n",
    "    zero_one_list2=query_word_zero_one[1]\n",
    "    # implement and using '&'\n",
    "    if word == 'and':\n",
    "        bitwise_logic=[l1 & l2 for (l1,l2) in zip(zero_one_list1,zero_one_list2)]\n",
    "        query_word_zero_one.remove(zero_one_list1)\n",
    "        query_word_zero_one.remove(zero_one_list2)\n",
    "        query_word_zero_one.insert(0,bitwise_logic)\n",
    "    # implement or using '|'\n",
    "    elif word == 'or':\n",
    "        bitwise_logic=[l1 | l2 for (l1,l2) in zip(zero_one_list1,zero_one_list2)]\n",
    "        query_word_zero_one.remove(zero_one_list1)\n",
    "        query_word_zero_one.remove(zero_one_list2)\n",
    "        query_word_zero_one.insert(0,bitwise_logic)\n",
    "    # implement not using 'not'\n",
    "    elif word == 'not':\n",
    "        bitwise_logic=[int(not l1 == True) for l1 in zero_one_list2]\n",
    "        query_word_zero_one.remove(zero_one_list2)\n",
    "        query_word_zero_one.remove(zero_one_list1)\n",
    "        bitwise_logic=[l1 & l2 for (l1,l2) in zip(zero_one_list1,bitwise_logic)]\n",
    "    query_word_zero_one.insert(0,bitwise_logic)\n",
    "# except IndexError:\n",
    "    # print()\n",
    "        \n",
    "query_files_result=list()\n",
    "try:\n",
    "    for i,zero_one in enumerate(query_word_zero_one[0]):\n",
    "        if zero_one==1:\n",
    "            query_files_result.append(indexed_files[i+1]) # recall indexed_files is a dict\n",
    "    print(f'{query_input} is present in {len(query_files_result)} files and they are \\n {query_files_result}')\n",
    "except IndexError:\n",
    "    print(f'No files for query > {query_input}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37088"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_words_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watchmak'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter=PorterStemmer()\n",
    "porter.stem('Watchmaker')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "365d70965140afb04a698773bfdd31483bc82432b779112c2a78b5de7c16d125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
