{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to keep these words unchangable we use python set\n",
    "# as our docs are in english we use english stopwords\n",
    "Stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_word_freq(word_list):\n",
    "    '''\n",
    "    takes word list and returns unique-words with their freq in dict\n",
    "    '''\n",
    "    unqiue_words=list()\n",
    "    words_freq={}\n",
    "    # store unqiue words in list\n",
    "    for word in word_list:\n",
    "        if word not in unqiue_words:\n",
    "            unqiue_words.append(word)\n",
    "    # get unique words freq from all words list using count\n",
    "    for unqiue_word in unqiue_words:\n",
    "        # NOTE: this same method can be used to get freq of a particular word in given document\n",
    "        words_freq[unqiue_word]=word_list.count(unqiue_word) # used count method of lists\n",
    "    \n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"to init a node with docID and word_freq in that document\"\"\"\n",
    "\n",
    "    def __init__(self,doc_id,word_freq=None):\n",
    "        self.doc_id=doc_id\n",
    "        self.word_freq=word_freq\n",
    "        self.next=None\n",
    "\n",
    "class LinkedList:\n",
    "    '''\n",
    "    init linked list class\n",
    "    '''\n",
    "    def __init__(self,head=None):\n",
    "        self.head=head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpora_dir='test-corpora'\n",
    "english_corpora_dir='english-corpora'\n",
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:20: DeprecationWarning: invalid escape sequence \\s\n",
      "<>:29: DeprecationWarning: invalid escape sequence \\d\n"
     ]
    }
   ],
   "source": [
    "words_global=list() # to store words from all docs\n",
    "words_freq_global_dict={} # to store word:freq from all docs\n",
    "indexed_files={}\n",
    "\n",
    "for i,file in enumerate(os.listdir(test_corpora_dir)):\n",
    "    with open(test_corpora_dir+'/'+file,'r') as f:\n",
    "        corpora_file_str=f.read() # text str\n",
    "\n",
    "    \"\"\"remove css code lines\"\"\"\n",
    "    css_regex=re.compile(r'.mw.*}')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=css_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove html tag lines\"\"\"\n",
    "    html_regex=re.compile(r'<.*>')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=html_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove special characters and only keep a-z;A-Z;0-9;space\"\"\"\n",
    "    '''\n",
    "    to avoid deprecation warning used one more backslash to escape backslash\n",
    "    so \\s -> \\\\s\n",
    "    '''\n",
    "    special_regex=re.compile('[^a-zA-Z0-9\\\\s]')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=special_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove digits for file\"\"\"\n",
    "    digit_regex=re.compile('\\d')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=digit_regex.sub('',corpora_file_str)\n",
    "\n",
    "    sentence_tokens=sent_tokenize(corpora_file_str)\n",
    "    # print(len(sentence_tokens))\n",
    "    word_tokens=word_tokenize(corpora_file_str)\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    \"\"\"avoid single characters and lower them and remove stopwords\"\"\"\n",
    "    # TODO: what about special cases like UP > up or PIN > pin \n",
    "    # TODO: incase of tf-idf stopwords do not matter. \n",
    "    # for this should keep them? for cases \"like to be or not to be\"\n",
    "    word_tokens=[porter.stem(word.lower()) for word in word_tokens if len(word)>1 and word not in Stopwords]\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    words_freq_global_dict.update(get_unique_word_freq(word_tokens)) # used update method of dict\n",
    "\n",
    "    indexed_files[i+1]=file\n",
    "\n",
    "unique_words_global=set(words_freq_global_dict.keys()) # used set to keep words unchangable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create inverted index linked list with freq count\"\"\"\n",
    "inverted_index_data={}\n",
    "for word in unique_words_global:\n",
    "    inverted_index_data[word]=LinkedList()\n",
    "    inverted_index_data[word].head=Node(1,Node)\n",
    "\n",
    "\"\"\"to use in pivot length normalization we store # of unique terms in a doc\"\"\"\n",
    "unique_words_in_doc={}\n",
    "\n",
    "\"\"\"doc lengths are stored for use in BM25\"\"\"\n",
    "doc_lengths={}\n",
    "\n",
    "for i,file in enumerate(os.listdir(test_corpora_dir)):\n",
    "    with open(test_corpora_dir+'/'+file,'r') as f:\n",
    "        corpora_file_str=f.read() # text str\n",
    "\n",
    "    \"\"\"remove css code lines\"\"\"\n",
    "    css_regex=re.compile(r'.mw.*}')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=css_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove html tag lines\"\"\"\n",
    "    html_regex=re.compile(r'<.*>')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=html_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove special characters and only keep a-z;A-Z;0-9;space\"\"\"\n",
    "    '''\n",
    "    to avoid deprecation warning used one more backslash to escape backslash\n",
    "    so \\s -> \\\\s\n",
    "    '''\n",
    "    special_regex=re.compile('[^a-zA-Z0-9\\\\s]')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=special_regex.sub('',corpora_file_str)\n",
    "\n",
    "    \"\"\"remove digits for file\"\"\"\n",
    "    digit_regex=re.compile('\\d')\n",
    "    # substitue regex expression by ''\n",
    "    corpora_file_str=digit_regex.sub('',corpora_file_str)\n",
    "\n",
    "    sentence_tokens=sent_tokenize(corpora_file_str)\n",
    "    # print(len(sentence_tokens))\n",
    "    word_tokens=word_tokenize(corpora_file_str)\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    \"\"\"avoid single characters and lower them and remove stopwords\"\"\"\n",
    "    # TODO: what about special cases like UP > up or PIN > pin \n",
    "    # TODO: incase of tf-idf stopwords do not matter. \n",
    "    # for this should keep them? for cases \"like to be or not to be\"\n",
    "    word_tokens=[porter.stem(word.lower()) for word in word_tokens if len(word)>1 and word not in Stopwords]\n",
    "    # print(len(word_tokens))\n",
    "\n",
    "    tmp_word_freq_of_doc=get_unique_word_freq(word_tokens)\n",
    "    unique_words_in_doc[i+1]=len(tmp_word_freq_of_doc)\n",
    "    doc_lengths[i+1]=sum(tmp_word_freq_of_doc.values())\n",
    "\n",
    "    for word in tmp_word_freq_of_doc.keys():\n",
    "        tmp_LinkedList=inverted_index_data[word].head\n",
    "        while tmp_LinkedList.next is not None:\n",
    "            tmp_LinkedList=tmp_LinkedList.next\n",
    "        tmp_LinkedList.next=Node(i+1,tmp_word_freq_of_doc[word])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('inverted-index-data.json','r') as file:\n",
    "        inverted_index_data=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not-sunil', 'prem', 'james', 'bond', 'not-ram', 'shyam']\n",
      "['and', 'and', 'and', 'or', 'and']\n"
     ]
    }
   ],
   "source": [
    "## a simple check system to check 'not' system\n",
    "tokenized_query=['not','sunil','prem','james','bond','or','not','ram','shyam']\n",
    "bool_words=list()\n",
    "search_words=list()\n",
    "for i,word in enumerate(tokenized_query):\n",
    "        if word != \"and\" and word != \"or\":\n",
    "            if word == 'not':\n",
    "                search_words.append(f'not-{tokenized_query[i+1]}')\n",
    "            else:\n",
    "                if tokenized_query[i-1]!='not':\n",
    "                    search_words.append(word)\n",
    "                if i!=len(tokenized_query)-1 and tokenized_query[i+1] != 'and' and tokenized_query[i+1]!='or':\n",
    "                     bool_words.append('and')\n",
    "        else:   \n",
    "            bool_words.append(word)\n",
    "print(search_words)\n",
    "print(bool_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javascript and python is present in 6 files and they are \n",
      " ['C00005.txt', 'C00001.txt', 'C00007.txt', 'C00006.txt', 'C00003.txt', 'C00002.txt']\n"
     ]
    }
   ],
   "source": [
    "query_input=input('Query > ')\n",
    "tokenized_query=[word.lower() for word in word_tokenize(query_input)]\n",
    "\n",
    "\n",
    "\"\"\"seperate logic and search terms\"\"\"\n",
    "bool_words=list()\n",
    "search_words=list()\n",
    "\n",
    "for word in tokenized_query:\n",
    "    if word != \"and\" and word != \"or\" and word != \"not\":\n",
    "        search_words.append(word)\n",
    "    else:\n",
    "        bool_words.append(word)\n",
    "\n",
    "search_words=[porter.stem(word) for word in search_words]\n",
    "total_documents=len(indexed_files)\n",
    "\n",
    "query_word_zero_one=list()\n",
    "for word in search_words:\n",
    "    if word in unique_words_global:\n",
    "        tmp_zero_one=[0]*total_documents\n",
    "        curr_linkedlist=inverted_index_data[word].head\n",
    "        while curr_linkedlist.next is not None:\n",
    "            tmp_zero_one[curr_linkedlist.next.doc_id-1]=1\n",
    "            curr_linkedlist=curr_linkedlist.next\n",
    "        query_word_zero_one.append(tmp_zero_one)\n",
    "    else:\n",
    "        print(f'word > {word} < is not found in any document')\n",
    "        # sys.exit() \n",
    "\"\"\"create a merged boolean(zero-one) list using bitwise operations\"\"\"\n",
    "# try:\n",
    "for word in bool_words:\n",
    "    zero_one_list1=query_word_zero_one[0]\n",
    "    zero_one_list2=query_word_zero_one[1]\n",
    "    # implement and using '&'\n",
    "    if word == 'and':\n",
    "        bitwise_logic=[l1 & l2 for (l1,l2) in zip(zero_one_list1,zero_one_list2)]\n",
    "        query_word_zero_one.remove(zero_one_list1)\n",
    "        query_word_zero_one.remove(zero_one_list2)\n",
    "        query_word_zero_one.insert(0,bitwise_logic)\n",
    "    # implement or using '|'\n",
    "    elif word == 'or':\n",
    "        bitwise_logic=[l1 | l2 for (l1,l2) in zip(zero_one_list1,zero_one_list2)]\n",
    "        query_word_zero_one.remove(zero_one_list1)\n",
    "        query_word_zero_one.remove(zero_one_list2)\n",
    "        query_word_zero_one.insert(0,bitwise_logic)\n",
    "    # implement not using 'not'\n",
    "    elif word == 'not':\n",
    "        bitwise_logic=[int(not l1 == True) for l1 in zero_one_list2]\n",
    "        query_word_zero_one.remove(zero_one_list2)\n",
    "        query_word_zero_one.remove(zero_one_list1)\n",
    "        bitwise_logic=[l1 & l2 for (l1,l2) in zip(zero_one_list1,bitwise_logic)]\n",
    "    query_word_zero_one.insert(0,bitwise_logic)\n",
    "# except IndexError:\n",
    "    # print()\n",
    "        \n",
    "query_files_result=list()\n",
    "try:\n",
    "    for i,zero_one in enumerate(query_word_zero_one[0]):\n",
    "        if zero_one==1:\n",
    "            query_files_result.append(indexed_files[i+1]) # recall indexed_files is a dict\n",
    "    print(f'{query_input} is present in {len(query_files_result)} files and they are \\n {query_files_result}')\n",
    "except IndexError:\n",
    "    print(f'No files for query > {query_input}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ======\n",
      "1 3 4 5 6 7 8 9 ======\n",
      "3 ======\n",
      "6 ======\n",
      "1 2 3 4 5 6 7 8 9 ======\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(unique_words_global):\n",
    "    if i<5:\n",
    "        l=inverted_index_data[word].head\n",
    "        while l.next is not None:\n",
    "            print(l.next.doc_id,end=' ')\n",
    "            l=l.next\n",
    "        print('======')\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"implement tf-idf using simple/basic definition\"\"\"\n",
    "\n",
    "def get_tf_idf(term,document_id):\n",
    "    '''\n",
    "    takes term and doc-id\n",
    "    and return tf-idf for term and document\n",
    "    If doc_id is not in doc_ids then it is zero\n",
    "    '''\n",
    "    doc_ids=list()\n",
    "    # word_freqs=list()\n",
    "\n",
    "    # get term appearning doc-ids using inverted index linked list\n",
    "    tmp=inverted_index_data[term].head\n",
    "    while tmp.next is not None:\n",
    "        doc_ids.append(tmp.next.doc_id)\n",
    "        # since no need to extract word freq for all documents in which the term appears\n",
    "        if doc_ids[-1]==document_id:\n",
    "            term_freq_in_doc=tmp.next.word_freq\n",
    "        tmp=tmp.next\n",
    "    if document_id not in doc_ids:\n",
    "        return [0,0]\n",
    "    else:\n",
    "        idf=np.log(total_documents/len(doc_ids))\n",
    "        # tf=term_freq_in_doc # since tf is just simple count of how many times term appears in particular document\n",
    "\n",
    "        return [term_freq_in_doc*idf,term_freq_in_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_input=input('Query > ')\n",
    "processed_query=[porter.stem(word.lower()) for word in word_tokenize(query_input)]\n",
    "# print(processed_query)\n",
    "doc_scores={}\n",
    "for doc in indexed_files.keys():\n",
    "    tmp_tf_idfs=[]\n",
    "    for word in processed_query:\n",
    "        foo,faa=get_tf_idf(word,doc)\n",
    "        tmp_tf_idfs.append(foo)\n",
    "\n",
    "    doc_scores[doc]=sum(tmp_tf_idfs)/unique_words_in_doc[doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7024/2585890831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_doc_score_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_scores' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_doc_score_dict=dict(sorted(doc_scores.items(), key=lambda item: item[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For \"javascript and python\" top five docs are:\n",
      "1. C00005.txt\n",
      "2. C00001.txt\n",
      "3. C00002.txt\n",
      "4. C00007.txt\n",
      "5. C00009.txt\n"
     ]
    }
   ],
   "source": [
    "print(f'For \"{query_input}\" top five docs are:')\n",
    "for i,doc_id in enumerate(sorted_doc_score_dict.keys()):\n",
    "    if i<5:\n",
    "        print(f'{i+1}. {indexed_files[doc_id]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bm25 implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=0.75\n",
    "k1=1.5\n",
    "# avg document length\n",
    "avdl=sum(doc_lengths.values())/total_documents\n",
    "\n",
    "# query_input=input('Query > ')\n",
    "processed_query=[porter.stem(word.lower()) for word in word_tokenize(query_input)]\n",
    "# print(processed_query)\n",
    "doc_scores={}\n",
    "for doc in indexed_files.keys():\n",
    "    tmp_tf_idfs=[]\n",
    "    tmp_tfs=[]\n",
    "    for word in processed_query:\n",
    "        foo,faa=get_tf_idf(word,doc) # foo: tf-idf, faa: tf\n",
    "        tmp_tf_idfs.append(foo)\n",
    "        tmp_tfs.append(faa)\n",
    "\n",
    "    # for RSV formula please see markdowns\n",
    "    doc_scores[doc]=sum([((k1+1)*i)/((k1*(1-b+(b*(doc_lengths[doc]/avdl))))+j) for i,j in zip(tmp_tf_idfs,tmp_tfs)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For \"javascript and python\" top five docs are:\n",
      "1. C00002.txt\n",
      "2. C00005.txt\n",
      "3. C00007.txt\n",
      "4. C00001.txt\n",
      "5. C00003.txt\n"
     ]
    }
   ],
   "source": [
    "sorted_doc_score_dict=dict(sorted(doc_scores.items(), key=lambda item: item[1],reverse=True))\n",
    "print(f'For \"{query_input}\" top five docs are:')\n",
    "for i,doc_id in enumerate(sorted_doc_score_dict.keys()):\n",
    "    if i<5:\n",
    "        print(f'{i+1}. {indexed_files[doc_id]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: pre-process query also as we pre-process corpus for tf-idf and BM25 case\n",
    "- TODO: how to rank for boolean model?\n",
    "- TODO: how to find relevance from scores"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "365d70965140afb04a698773bfdd31483bc82432b779112c2a78b5de7c16d125"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
